{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yukon Chinook Environmental Data Extraction for Forecast\n",
    "## Purpose\n",
    "The Yukon River Delta Chinook Salmon run timing and outlook depend on the environmental conditions in the Bering Sea each year. The purpose of this notebook is to extract that data, do some minor processing and/or derivations, and save the time-series of the outputs as CSV files.\n",
    "\n",
    "## Inputs\n",
    "The timing model is dependent on the following environmental conditions:\n",
    "\n",
    "### Mean Sea Ice Concentration, March 20 - June 1\n",
    "- \"Average Sea Ice Concentration (%) Shpanberg Strait (62–63°N, 166–169°W)\"\n",
    "- Extracted from the NSIDC Near-Real-Time DMSP SSMIS Daily Polar Gridded Sea Ice Concentrations, Version 1\n",
    "- Original: https://nsidc.org/data/NSIDC-0081/versions/1\n",
    "- AOOS: https://portal.aoos.org/#module-metadata/391183ee-827e-11e1-a4f3-00219bfe5678/c4d14166-cae8-4bb0-8cd5-fc876f07d63c\n",
    "- RW Directory: `/data/packrat/nsids/nsidc0081/processed/2023/`\n",
    "\n",
    "### Mean Marine Surface Temperature, May 1 - May 31\n",
    "\n",
    "- \"Marine Surface Temperature (°C) 26.5 mi due west of Nunaktuk Island (Middle Mouth) (63.1°N 165.5°W)\"- unlike Sea Ice, this is just extracted from a single location.\n",
    "- Extracted from NCEP/NCAR Reanalysis 1: Surface Flux dataset\n",
    "- Original: https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.surfaceflux.html\n",
    "- AOOS: https://portal.aoos.org/#module-metadata/071705de-8400-11e1-99fe-00219bfe5678/047e7934-2ed7-431f-8a85-0bdd8c6a08cd\n",
    "- RW Directory: `/data/packrat/ncep/reanalysis/dailyavgs/surface_gauss/processed/2023/2023_01/surface_gauss_2023.nc`\n",
    "\n",
    "### Air Temperature in Nome, Alaska, April 1 - April 30\n",
    "\n",
    "- \"Air Temperature (°C) for Nome, AK\"\n",
    "- Recorded by National Weather Service station PAOM\n",
    "- Original: https://w1.weather.gov/data/obhistory/PAOM.html\n",
    "- AOOS: https://portal.aoos.org/#metadata/14059/station\n",
    "- AOOS ERDDAP: http://erddap.aoos.org/erddap/tabledap/gov_noaa_nws_paom.html\n",
    "- Note: there are multiple weather stations in Nome. Other options include a [Marine Exchange of Alaska weather station](https://portal.aoos.org/#metadata/103371/station) and a [CO-OPS station](https://portal.aoos.org/#metadata/12017/station).\n",
    "\n",
    "## Outputs\n",
    "This notebook can be used to get the following:\n",
    "- CSV extractions of the environmental data for the given periods.\n",
    "\n",
    "## Modification History\n",
    "- 2021.05.13: Protyping WIP (W. Koeppen, Axiom)\n",
    "- 2023.05.05: Updating file paths (Aidan Lewis, Axiom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the year will propogate through all three requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some configurations. I didn't use all of these, but it's kind of useful to collect these here anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"sea_ice\": {\n",
    "        \"title\": \"NSIDC Sea Ice\",\n",
    "        \"path\": \"/data/packrat/nsidc/nsidc0081/processed/2023/\",\n",
    "        \"data_varname\": \"Sea_Ice_Concentration\",\n",
    "        \"lon_varname\": \"longitude\",\n",
    "        \"lat_varname\": \"latitude\",\n",
    "        \"time_varname\": \"time\",\n",
    "    },\n",
    "     \"surface_temp\": {\n",
    "         \"title\": \"NCEP Surface Temperature\",\n",
    "         \"path\": \"/data/packrat/ncep/reanalysis/dailyavgs/surface_gauss/processed/2023/2023_01/\",\n",
    "         \"data_varname\": \"skt\",\n",
    "         \"lon_varname\": \"lon\",\n",
    "         \"lat_varname\": \"lat\",\n",
    "         \"time_varname\": \"time\",\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indexes the files on disk, listing the file location, time available, and integer index of each time. You don't have to do this every time you want a new year. But you'll have to do it at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_file_index(search_string, time_var):\n",
    "    \n",
    "    filenames = sorted(glob.glob(search_string))\n",
    "    nfiles = len(filenames)\n",
    "    \n",
    "    # Build a tuple with dates and filenames the file contains for every file in the index\n",
    "    time_file = []\n",
    "\n",
    "    for i in trange(nfiles):\n",
    "\n",
    "        with netCDF4.Dataset(filenames[i]) as netcdf:\n",
    "            # extract the time, turn it into a date\n",
    "            \n",
    "            time_dat = netcdf.variables[time_var]\n",
    "            times = np.array(time_dat)\n",
    "            \n",
    "            # some have calendar, some don't\n",
    "            try:\n",
    "                times = netCDF4.num2date(times, time_dat.units, time_dat.calendar, only_use_python_datetimes=True)\n",
    "            except:\n",
    "                times = netCDF4.num2date(times, time_dat.units)\n",
    "        \n",
    "        for j in range(len(times)):\n",
    "            time_file.append((times[j], filenames[i], j))\n",
    "    \n",
    "    \n",
    "    result = pd.DataFrame(time_file, columns=['date', 'file', 't_index'])\n",
    "    time_format=\"%Y-%m-%d %H:%M:%S\"\n",
    "    result['date'] = [pd.Timestamp(t.strftime(time_format)) for t in result['date']]\n",
    "    result = result.set_index('date')\n",
    "\n",
    "    #check for duplicates\n",
    "    dupes = result.index.duplicated(keep='first')\n",
    "    \n",
    "    if dupes.sum() > 0:\n",
    " #       print('Found duplicate times, using first one found.')\n",
    "        result = result[~dupes]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSIDC Sea Ice\n",
    "A few notes here:\n",
    "\n",
    "- This method select only the points that have latitude and longitudes within our bounds. This dataset provides the centroids of the cells, so this method won't take into account partial pixels. A cell is either in or out.\n",
    "- The data is projected, so we can't select centroids by their integer location. Instead we select them by their latitude and longitude labels.\n",
    "- Xarray throws a warning when opening these files: `SerializationWarning: variable 'Sea_Ice_Concentration' has multiple fill values {-9999, 255}, decoding all values to NaN.` It's not a problem for this work, so I supress these with a warnings catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:01<00:00, 81.68it/s]\n"
     ]
    }
   ],
   "source": [
    "ice_time_file = time_file_index(os.path.join(data_config[\"sea_ice\"][\"path\"], '**/*.nc'), data_config[\"sea_ice\"][\"time_varname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_file_list = ice_time_file.loc[f\"{year}-03-20\":f\"{year}-06-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:04<00:00, 16.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_ice_values(row):\n",
    "    \n",
    "    \"\"\"Get the mean Sea Ice Concentration value in a lat/lon box.\"\"\"\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=xr.SerializationWarning)\n",
    "        dat = xr.open_dataset(row['file']).isel(time=row['t_index'])\n",
    "    \n",
    "    masked = dat['Sea_Ice_Concentration'].\\\n",
    "        where(dat['latitude'] >= 62).\\\n",
    "        where(dat['latitude'] <= 63).\\\n",
    "        where(dat['longitude'] >= -169).\\\n",
    "        where(dat['longitude'] <= -166)\n",
    "    subset = masked.values[~np.isnan(masked.values)]\n",
    "    result = np.nanmean(subset)\n",
    "    \n",
    "    return result\n",
    "\n",
    "vals = []\n",
    "for i in trange(len(ice_file_list)):\n",
    "    vals.append(get_ice_values(ice_file_list.iloc[i]))\n",
    "\n",
    "ice_df = pd.DataFrame({\"mean_sea_ice_percent\":vals}, index=ice_file_list.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_df.to_csv(f\"../outputs/sea_ice_concentration-{year}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCEP Skin Temperature\n",
    "Notes:\n",
    "- longitudes here are from 0 to 360 positive east. So 63.1°N 165.5°W becomes 63.1°N 194.5°E\n",
    "- there are no exact matches here, but we get pretty close (centroid is at 63.8079°N 195.0°E)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "skt_time_file = time_file_index(os.path.join(data_config[\"surface_temp\"][\"path\"], '*.nc'), data_config[\"surface_temp\"][\"time_varname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skt_file_list = skt_time_file.loc[f\"{year}-05-01\":f\"{year}-05-31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 18.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_skt_values(row):\n",
    "    \n",
    "    \"\"\"Get the skin temperature value at the lat/lon.\"\"\"\n",
    "    \n",
    "    dat = xr.open_dataset(row['file']).isel(time=row['t_index'])\n",
    "    \n",
    "    lat_index = find_nearest(dat['lat'], 63.1)\n",
    "    lon_index = find_nearest(dat['lon'], 194.5)\n",
    "    \n",
    "    result = dat[\"skt\"].isel(lat=lat_index, lon=lon_index).item()\n",
    "    \n",
    "    return result\n",
    "\n",
    "vals = []\n",
    "for i in trange(len(skt_file_list)):\n",
    "    vals.append(get_skt_values(skt_file_list.iloc[i]))\n",
    "\n",
    "skt_df = pd.DataFrame({\"surface_temp_c\":vals}, index=skt_file_list.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skt_df.to_csv(f\"../outputs/surface_temperature-{year}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nome Air Temperature\n",
    "Let's just get this from [AOOS ERDDAP](http://erddap.aoos.org/erddap/index.html).\n",
    "- This station provides hourly data in Celsius.\n",
    "- We can re-sample to daily to match the other datasets. But I'll add another column that is the count of the number of values in each daily mean, so we know if there are missing data.\n",
    "- Data in our archive begins on May 16, 2015, so this will fail if years before that are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erddap_sensor(\n",
    "    stationurl,\n",
    "    time_min=datetime(2020,9,1,0,0),\n",
    "    time_max=datetime(2020,12,31,0,0)\n",
    "):\n",
    "    \n",
    "    url = f\"{stationurl}.csv\" \\\n",
    "    f\"?time,latitude,longitude,air_temperature\" \\\n",
    "    f\"&time>={time_min.strftime('%Y-%m-%dT%H:%M:%SZ')}\" \\\n",
    "    f\"&time<={time_max.strftime('%Y-%m-%dT%H:%M:%SZ')}\"\n",
    "            \n",
    "    print(url)\n",
    "    result = pd.read_csv(url, parse_dates=True, index_col='time', skiprows=[1])\n",
    "    result = result.sort_index() # put it in order\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://erddap.aoos.org/erddap/tabledap/gov_noaa_awc_paom.csv?time,latitude,longitude,air_temperature&time>=2023-04-01T00:00:00Z&time<=2023-05-01T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "request_data = erddap_sensor(\n",
    "    \"http://erddap.aoos.org/erddap/tabledap/gov_noaa_awc_paom\",\n",
    "    time_min=datetime(year,4,1,0,0),\n",
    "    time_max=datetime(year,5,1,0,0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample = request_data[\"air_temperature\"].resample('1D')\n",
    "\n",
    "nome_df = pd.DataFrame({\n",
    "    \"air_temperature_c\": resample.mean(),\n",
    "    \"n_observations\": resample.count()\n",
    "    \n",
    "}, index=resample.mean().index)\n",
    "nome_df.index.name = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_df.to_csv(f\"../outputs/nome_air_temp-{year}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}